{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143780a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3de73",
   "metadata": {},
   "source": [
    "#Image I/O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c92f20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# imagenet mean and std for normalization\n",
    "VGG_MEAN = [0.485, 0.456, 0.406]\n",
    "VGG_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_image(path, max_size=512):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    # downsampling for maintaining size\n",
    "    if max(img.size) > max_size:\n",
    "        scale = max_size / max(img.size)\n",
    "        new_size = [int(img.width * scale), int(img.height * scale)]\n",
    "        img = img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "    return img\n",
    "\n",
    "# make sure img is tensor with the right dimension\n",
    "def preprocess(img: Image.Image, device):\n",
    "    # convert to tensor\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=VGG_MEAN, std=VGG_STD)\n",
    "    ])\n",
    "\n",
    "    # add batch dimension [1, 3, H, W]\n",
    "    x = tfm(img).unsqueeze(0).to(device)   # add one dimension on index 0\n",
    "    return x\n",
    "\n",
    "# convert back to viewable image\n",
    "def deprocess(x: torch.Tensor):\n",
    "    # match the mean and std with dim of img\n",
    "    mean = torch.tensor(VGG_MEAN, device=x.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(VGG_STD, device=x.device).view(1, 3, 1, 1)\n",
    "\n",
    "    # denorm\n",
    "    y = x * std + mean\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "    y = y.squeeze(0).detach().cpu()\n",
    "\n",
    "    return transforms.ToPILImage()(y)\n",
    "\n",
    "def save_image(x: torch.Tensor, out_path: str):\n",
    "    deprocess(x).save(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9bbc4",
   "metadata": {},
   "source": [
    "#VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ad724",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# define layers needed from VGG_19\n",
    "VGG_LAYERS = {\n",
    "    \"conv1_1\": 0,\n",
    "    \"conv2_1\": 5,\n",
    "    \"conv3_1\": 10,\n",
    "    \"conv4_1\": 19,\n",
    "    \"conv4_2\": 21,    # used for content layer in the original paper\n",
    "    \"conv5_1\": 28,\n",
    "}\n",
    "\n",
    "class VGG19FeatureExtractor(nn.Module):\n",
    "    def __init__(self, layer_map=VGG_LAYERS):\n",
    "        super().__init__()\n",
    "\n",
    "        # load vgg19, which was pretrained for ImageNet, and discard the fully connected layers\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "\n",
    "        # turn off ReLU inplce to prevent changing the input of conv layers/ replace maxpooling with avgpooling\n",
    "        for i,m in enumerate(vgg):\n",
    "            if isinstance(m, nn.ReLU):\n",
    "                vgg[i] = nn.ReLU(inplace=False)\n",
    "            if isinstance(m, nn.MaxPool2d):\n",
    "                vgg[i] = nn.AvgPool2d(kernel_size=m.kernel_size, stride=m.stride, padding=m.padding)\n",
    "\n",
    "        # turn on the evaluation mode\n",
    "        self.features = vgg.eval()\n",
    "\n",
    "        # freeze the model from being trained\n",
    "        for p in self.features.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.layer_map = layer_map\n",
    "\n",
    "    # cpu or gpu\n",
    "    @torch.no_grad()\n",
    "    def device(self):\n",
    "        return next(self.features.parameters()).device\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B, 3, H, W] in nomrlized VGG space. Returns dict(name: activation)\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        t = x\n",
    "\n",
    "        for i, layer in enumerate(self.features):\n",
    "            t = layer(t)\n",
    "            for name, idx in self.layer_map.items():\n",
    "                if i == idx:\n",
    "                    feats[name] = t\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e1885",
   "metadata": {},
   "source": [
    "# Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10270d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def gram_matrix(feat: torch.Tensor):\n",
    "    B, C, H, W = feat.shape\n",
    "    F = feat.view(B, C, H*W)\n",
    "    G = F @ F.transpose(1, 2)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716dbe6",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb31b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def content_loss(gen_feats: dict, content_feats: dict, layer: str=\"conv4_2\"):\n",
    "    F_l = gen_feats[layer]\n",
    "    P_l = content_feats[layer]\n",
    "\n",
    "    return 0.5 * F.mse_loss(F_l, P_l, reduction=\"sum\")\n",
    "\n",
    "def style_loss(gen_feats: dict, style_feats: dict, style_layers: list[str], layer_weights: dict):\n",
    "    if layer_weights is None:\n",
    "        layer_weights = {l: 1.0 for l in style_layers}\n",
    "\n",
    "    total = 0.0\n",
    "\n",
    "    for layer in style_layers:\n",
    "        F_l = gen_feats[layer]\n",
    "        S_l = style_feats[layer]\n",
    "        _, C, H, W = F_l.shape\n",
    "        N_l = C\n",
    "        M_l = H * W\n",
    "\n",
    "        G_l = gram_matrix(F_l)\n",
    "        A_l = gram_matrix(S_l)\n",
    "\n",
    "        E_l = ((G_l - A_l)**2).sum() / (4 * N_l**2 * M_l**2)\n",
    "\n",
    "        w_l = layer_weights.get(layer, 1.0)\n",
    "        total = total + w_l * E_l\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4a7b4",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e72585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "content_path = \"content_image.jpg\"\n",
    "style_path = \"style_image.jpg\"\n",
    "\n",
    "style_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
    "layer_weights = {l: 0.2 for l in style_layers}\n",
    "\n",
    "alpha = 1.0 # content weight\n",
    "beta = 1e5 # style weight\n",
    "\n",
    "content_img = load_image(content_path)\n",
    "style_img = load_image(style_path)\n",
    "\n",
    "content = preprocess(content_img, device=device) # [1, 3, H, W]\n",
    "style = preprocess(style_img, device=device)\n",
    "\n",
    "vgg = VGG19FeatureExtractor().to(device)\n",
    "vgg.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    content_feats = vgg(content)\n",
    "    style_feats = vgg(style)\n",
    "\n",
    "generated = content.clone().requires_grad_(True) # start from content\n",
    "\n",
    "optimizer = optim.LBFGS([generated])\n",
    "num_steps = 1000\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  gen_feats = vgg(generated)\n",
    "  c_loss = content_loss(gen_feats, content_feats, layer=\"conv4_2\")\n",
    "  s_loss = style_loss(gen_feats, style_feats, style_layers, layer_weights)\n",
    "\n",
    "  loss = alpha * c_loss + beta * s_loss\n",
    "  loss.backward()\n",
    "\n",
    "  return loss\n",
    "\n",
    "step = 0\n",
    "\n",
    "print(\"Optimizing...\")\n",
    "for step in range(num_steps):\n",
    "    loss = optimizer.step(closure)\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step+1}/{num_steps}, loss = {loss.item():.4f}\")\n",
    "\n",
    "out_img = deprocess(generated)\n",
    "out_img.save(\"nst_output.png\")\n",
    "display(out_img)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
